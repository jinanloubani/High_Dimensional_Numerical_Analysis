{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résolution de l'équation de Burgers à l'aide des méthodes GMRES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce travail, on se propose de simuler numériquement l'évolution de la solution d'une équation de Burgers, une équation aux dérivées partielles de dimension $1$. Plus précisément, nous voudrons simuler l'évolution de la solution de l'équation\n",
    "\n",
    "$$ \\left\\{\\begin{array}{ll}\\partial_t u(t,x) + u(t,x)\\partial_x u(t,x) = \\mu \\partial^2_x u(t,x), \\forall (t,x)\\in\\mathbb{R}^+\\times[0,1],\\\\ u(t,0) = u(t,1) = 0, \\forall t\\in\\mathbb{R}^+, \\\\ u(0,x) = v(x), \\forall x\\in[0,1],\\end{array}\\right.$$\n",
    "\n",
    "où $\\mu>0$ est un coefficient de viscosité et $v(x) = \\exp\\left(-\\frac{(x-x_0)^2}{d^2}\\right)$ où $d = 0.1$ et $x_0 = 1/4$. Il est à noter que les solutions de ce type d'équation ont la fâcheuse tendance à développer des singularités en temps finis lorsque $\\mu = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implémentation des méthodes GMRES sur un système simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par l'implémentation de la méthode GMRES. Pour cela, on va débuter avec un problème de taille $n = 50$ afin de tester les méthodes et voir leur comportement. Pour cela, on va considérer le problème suivant:\n",
    "\n",
    "$$ Ax=b,$$\n",
    "\n",
    "où $A$ sera une matrice aléatoire que l'on construira à l'aide de la fonction définie dessous et $b\\in\\mathbb{R}^n$ sera tel que $b_i = 1$, $\\forall i\\in\\{1,...,n\\}$. \n",
    "\n",
    "On veillera à ce que les méthodes soient appliquées via des fonctions qui prendront en entrée une matrice $A\\in M_{n,n}(\\mathbb{R})$ (taille quelconque), un vecteur $b\\in\\mathbb{R}^n$, un point de départ $x_0\\in\\mathbb{R}^n$ et une tolérance pour le critère d'arrêt (que l'on fixera selon la méthode)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0. Chargement des librairies classiques pour le calcul scientifique et une fonction utile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import numpy.linalg as npl\n",
    "import numpy.random as npr\n",
    "import scipy.linalg as spl\n",
    "import scipy.sparse as scs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Generation d'une matrice aléatoire creuse de densité \"d\" et ayant ses valeurs propres (réelles) tirées \n",
    "# aléatoirement selon une loi normale de moyenne \"m\" et de variance \"sigma\"\n",
    "def RandMat(n,d = 0.01,m = 1,sigma = 0.1):\n",
    "    ev = np.sqrt(sigma)*npr.randn(n) + m\n",
    "    D = np.diag(ev,0)\n",
    "    A = scs.rand(n,n,d)\n",
    "    A = scs.tril(A) + D\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Rappel sur GMRES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons à présent implémenter les méthodes GMRES et CGNR. Tout d'abord, nous nous intéressons à la méthode GMRES. On rappelle que le principe de la méthode GMRES consiste à résoudre un problème de moindres carrés par une factorisation QR à l'aide de l'algorithme de Givens. Afin de rendre l'approche plus simple, nous allons décomposer la méthode.\n",
    "\n",
    "Etant donné le résidu $r_0 = b - Ax_0$, la méthode GMRES consiste formellement à résoudre le problème de minimisation suivant, pour tout $k\\geq 1$,\n",
    "\n",
    "$$\n",
    "y_k = \\textrm{argmin}_{y\\in\\mathbb{R}^k}\\left\\| \\|r_0\\|_2e_{(k+1)}^1 - H_{(k+1,k)} y \\right\\|,\n",
    "$$\n",
    "\n",
    "où $e_{(k+1)}^1$ est le premier vecteur de la base canonique de $\\mathbb{R}^{k+1}$ et $H_{(k+1,k)}$ est la matrice de type Hessenberg obtenue par l'algorithme d'Arnoldi. Cette dernière vérifie l'équation\n",
    "\n",
    "$$\n",
    "AQ_k = Q_{k+1} H_{(k+1,k)},\n",
    "$$\n",
    "\n",
    "avec $Q_k$ la matrice associée à une base orthonormal de l'espace de Krylov $\\mathcal{K}_k(A,r_0)$. Une fois le vecteur $y_k$ calculé, une approximation de la solution du système linéaire est alors donnée par\n",
    "\n",
    "$$\n",
    "x_k = x_0 + Q_k y_k.\n",
    "$$\n",
    "\n",
    "Dans le notebook Krylov, vous avez implémenter la fonction `Step_Arnoldi` que nous donnons ci-dessous. On rappelle que cette fonction met en oeuvre l'ajout d'un vecteur $q^{k+1}$ à la base $Q_k$ et aussi l'ajout de la colonne $(h_{j,k})_{1\\leq j\\leq k+1}$ à la matrice $H_{(k,k-1)}$ pour $k\\geq 2$. On remarque que l'on ne fera pas la première étape et, donc, que le vecteur $q^1$ (qui est ici donc $r_0/\\|r_0\\|_2$) sera déjà donné par l'utilisateur. Les arguments d'entrée sont:\n",
    "- A: la matrice de la base de Krylov,\n",
    "- Q: la matrice correspondant à la base $Q_k$,\n",
    "- H: la matrice correspondant à la matrice $H_{(k,k-1)}$,\n",
    "\n",
    "et les arguments de sortie:\n",
    "- Q: la matrice correspondant à la base $Q_{k+1}$,\n",
    "- H: la matrice correspondant à la matrice $H_{(k+1,k)}$.\n",
    "\n",
    "Pour $k = 1$, la matrice $H_{(k,k-1)}$ n'est pas définie. On peut alors transmettre la variable d'entrée correspondante sous le format `None`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Step_Arnoldi(A,Q,H):\n",
    "    n,k = np.shape(Q)\n",
    "    h = np.zeros((k+1,1))\n",
    "    q = np.zeros((n,1))\n",
    "    p = A.dot(Q[:,-1].reshape((n,1)))\n",
    "    h[0:k,0] = Q.T.dot(p).squeeze()\n",
    "    p -= Q.dot(h[0:k,0].reshape(k,1))\n",
    "    h[k,0] = npl.norm(p)\n",
    "    if H is None:\n",
    "        H = h\n",
    "    else:\n",
    "        m = np.zeros((1,k-1))\n",
    "        H = np.concatenate((H,m),axis = 0)\n",
    "        H = np.concatenate((H,h),axis = 1)\n",
    "    p = p/h[k,0]\n",
    "    q[0:n,0] = p.squeeze()\n",
    "    Q = np.concatenate((Q,q),axis = 1)\n",
    "    return Q,H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Une première approche de GMRES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **A faire :** À l'aide de la fonction `npl.lstsq`, implémentez la fonction `GMRES_v0` qui prend en arguments d'entrée la matrice `A` et le vecteur `b` du système linéaire que l'on souhaite résoudre ainsi que le vecteur initial `x0`, la tolérance `tol` et le nombre maximal d'itérations `itermax`. Les arguments de sortie se borneront à la solution approximée `x` du système linéaire. ** Attention :** Prenez bien garde à la dimension des vecteurs qui seront sous la forme `(n,1)` où `n` est la longueur du vecteur (et non pas `(n,)`). On utilisera la fonction `reshape` pour parvenir à ce format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMRES_v0(A,b,x0 = None,tol = 1e-10,itermax = None):\n",
    "    if x0 is None:\n",
    "        x0 = np.zeros((len(b),1))\n",
    "    if itermax is None:\n",
    "        itermax = len(b)\n",
    "    b = b.reshape((len(b),1))\n",
    "    r = b - A.dot(x0)\n",
    "    beta = npl.norm(r)\n",
    "    rho = beta\n",
    "    Q = r.reshape((len(b),1))/float(beta)\n",
    "    H = None\n",
    "    niter = 1\n",
    "    while (niter<itermax) and (rho>tol):\n",
    "        Q,H = Step_Arnoldi(A,Q,H)\n",
    "        e1 = np.zeros(niter+1)\n",
    "        e1[0] = 1\n",
    "        y = npl.lstsq(H,e1*beta)[0]\n",
    "        y = y.reshape(niter,1)\n",
    "        x = x0 + Q[:,:niter].dot(y)\n",
    "        r = b - A.dot(x)\n",
    "        rho = npl.norm(r)\n",
    "        niter += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **A faire :** Testez votre code avec un système linéaire de taille 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = RandMat(50)\n",
    "b = np.random.random(50)\n",
    "x_exact = npl.solve(A,b)\n",
    "x = GMRES_v0(A,b)\n",
    "print(npl.norm(x-x_exact.reshape(50,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Résolution du problème de moindres carrés avec Givens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant nous passer de la boîte noire `npl.lstsq` en solutionnant le problème de moindres carrés par une factorisation QR de la matrice de Hessenberg `H`. Nous connaissons différentes approches afin d'obtenir une factorisation QR et nous allons ici nous baser sur la méthode de Givens puisque l'on veut simplement éliminer la sous-diagonale de la matrice `H`. On rappelle que, pour résoudre un problème de moindres carrés à l'aide d'une factorisation QR, on va factoriser la matrice `H`\n",
    "\n",
    "$$\n",
    "H = O R,\n",
    "$$\n",
    "\n",
    "où la matrice $O$ est orthogonal et la matrice $R$ est triangulaire supérieure. Dans le cadre de la méthode de Givens, l'application des rotations va se traduire par l'application de la matrice $O^T$ à la matrice $H$. Ainsi, pour résoudre un problème de moindres carrés de la forme\n",
    "\n",
    "$$\n",
    "\\min_{y\\in\\mathbb{R}^k}\\|e-Hy\\|_2,\n",
    "$$\n",
    "\n",
    "on va se borner à appliquer les rotations à la matrice $H$ et au vecteur $e$ pour obtenir le système\n",
    "\n",
    "$$\n",
    "O^T H y = O^Te \\quad\\Leftrightarrow\\quad R y = f.\n",
    "$$\n",
    "\n",
    "où $f = O^Te$. La matrice $R$ est ici de la même taille que $H$ mais sa dernière ligne est nulle. \n",
    "\n",
    "> **A faire :** Implémentez la fonction `QR_Givens` qui prendra en entrée la matrice `H` et le vecteur `e` et qui renvoie la matrice `R` et le vecteur `f` selon la méthode de Givens (c'est-à-dire à l'aide de rotations élémentaires dans le plan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QR_Givens(H,e):\n",
    "    n,m = np.shape(H)\n",
    "    e = e.reshape(len(e),1)\n",
    "    A = np.concatenate((H,e),axis = 1)\n",
    "    for k in range(m):\n",
    "        nm = np.sqrt(A[k,k]**2 + A[k+1,k]**2)\n",
    "        c = A[k,k]/nm\n",
    "        s = A[k+1,k]/nm\n",
    "        v_tmp = np.copy(A[k,:])\n",
    "        A[k,:] = c*A[k,:] + s*A[k+1,:]\n",
    "        A[k+1,:] = c*A[k+1,:] - s*v_tmp\n",
    "    R = A[:m+1,:m]\n",
    "    f = A[:,m]\n",
    "    return R,f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **A faire :** Testez votre code avec un problème de moindres carrés où la matrice est de type Hessenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.random((6,5))\n",
    "A = np.triu(A,-1)\n",
    "b = np.random.random(6)\n",
    "\n",
    "R,f = QR_Givens(A,b)\n",
    "x = npl.solve(R[:5,:5],f[:5])\n",
    "x_exact = npl.lstsq(A,b)[0]\n",
    "print(npl.norm(x_exact.squeeze() - x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **A faire :** Implémentez la fonction `GMRES_v1` qui reprend la fonction `GMRES_v0` mais où vous utilisez la fonction `QR_Givens` et non plus la fonction `npl.lstsq`. On pourra utiliser la fonction `npl.solve` pour résoudre le système triangulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMRES_v1(A,b,x0 = None,tol = 1e-10,itermax = None):\n",
    "    if x0 is None:\n",
    "        x0 = np.zeros((len(b),1))\n",
    "    if itermax is None:\n",
    "        itermax = len(b)\n",
    "    b = b.reshape((len(b),1))\n",
    "    r = b - A.dot(x0)\n",
    "    beta = npl.norm(r)\n",
    "    rho = beta\n",
    "    Q = r.reshape((len(b),1))/float(beta)\n",
    "    H = None\n",
    "    niter = 1\n",
    "    while (niter<itermax) and (rho>tol):\n",
    "        Q,H = Step_Arnoldi(A,Q,H)\n",
    "        e1 = np.zeros(niter+1)\n",
    "        e1[0] = 1\n",
    "        R,f = QR_Givens(H,e1*beta)\n",
    "        rho = abs(f[-1])\n",
    "        niter += 1\n",
    "    y = npl.solve(R[:niter-1,:niter-1],f[:niter-1]).reshape(niter-1,1)\n",
    "    x = x0 + Q[:,:niter-1].dot(y)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **A faire :** Testez votre code avec un système linéaire de taille 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = RandMat(50)\n",
    "b = np.random.random(50)\n",
    "x_exact = npl.solve(A,b)\n",
    "x = GMRES_v1(A,b)\n",
    "print(npl.norm(x-x_exact.reshape(50,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. La version optimisée de GMRES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons à présent optimiser les opérations de `GMRES_v1`. Une première étape va consister à ne plus stocker la matrice `H` obtenue par la fonction `Step_Arnoldi` mais uniquement `T`, la matrice triangulaire supérieure obtenue après la factorisation QR par la méthode de Givens. Pour cela, on va implémenter 2 nouvelles fonctions: \n",
    "\n",
    "- `Step_Arnoldi_h` qui va, à partir des matrices `A` et `Q`, donner la nouvelle matrice `Q` (avec un vecteur de la base de Krylov supplémentaire) et le vecteur `h` (et non plus la matrice `H`) qui correspond à la nouvelle colonne que l'on aurait obtenue dans la matrice `H` avec le procédé d'Arnoldi classique,\n",
    "- `QR_Givens_h` qui va prendre en argument une liste `O` qui contiendra les coefficients de rotations dans le plan, `h` un vecteur et `g` un autre vecteur. Cette fonction va, dans un premier temps, appliquer l'ensemble des rotations contenues dans `O` au vecteur `h`. Puis, dans un second temps, elle va calculer la rotation permettant d'éliminer le dernier coefficient du nouveau vecteur `h`, appliquer cette rotation au vecteur `h` et au vecteur `g` puis ajouter les coefficients de cette rotation à la liste `O`. On reprendra garde à gérer le cas où `O` est vide.\n",
    "\n",
    "> **A faire :** Implémenter les fonctions `Step_Arnoldi_h` et `QR_Givens_h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Step_Arnoldi_h(A,Q):\n",
    "    n,k = np.shape(Q)\n",
    "    h = np.zeros((k+1,1))\n",
    "    q = np.zeros((n,1))\n",
    "    p = A.dot(Q[:,-1].reshape((n,1)))\n",
    "    h[0:k,0] = Q.T.dot(p).squeeze()\n",
    "    p -= Q.dot(h[0:k,0].reshape(k,1))\n",
    "    h[k,0] = npl.norm(p)\n",
    "    p = p/h[k,0]\n",
    "    q[0:n,0] = p.squeeze()\n",
    "    Q = np.concatenate((Q,q),axis = 1)\n",
    "    return Q,h\n",
    "\n",
    "def QR_Givens_h(O,h,g):\n",
    "    if O is not None:\n",
    "        k = len(O)\n",
    "        for i in range(k):\n",
    "            cf_tmp = float(h[i])\n",
    "            h[i] = O[i][0]*h[i] + O[i][1]*h[i+1]\n",
    "            h[i+1] = O[i][0]*h[i+1] - O[i][1]*cf_tmp\n",
    "    nm = npl.norm(h[-2:])\n",
    "    c = float(h[-2])/nm\n",
    "    s = float(h[-1])/nm\n",
    "    cf_tmp = float(h[-2])\n",
    "    h[-2] = c*h[-2] + s*h[-1]\n",
    "    h[-1] = c*h[-1] - s*cf_tmp    \n",
    "    cf_tmp = float(g[-2])\n",
    "    g[-2] = c*g[-2] + s*g[-1]\n",
    "    g[-1] = c*g[-1] - s*cf_tmp  \n",
    "    if O is None:\n",
    "        O = [[c,s]]\n",
    "    else:\n",
    "        O += [[c,s]]\n",
    "    return O,h,g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **A faire :** Tester les fonctions avec les données suivantes et vérifier que vous obtenez les bonnes valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1.,5.],[-3.,2.]])\n",
    "b = np.array([1.,-1.]).reshape((2,1))\n",
    "g = np.array([npl.norm(b),0.])\n",
    "Q = b/npl.norm(b)\n",
    "O = None\n",
    "Q,h_tmp = Step_Arnoldi_h(A,Q)\n",
    "h = np.copy(h_tmp)\n",
    "O,h,g = QR_Givens_h(O,h,g)\n",
    "print(\"Vecteur h:\",h)\n",
    "print(\"Coefficients de rotations:\",O[0][0],\"et\",O[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut finalement passer à l'implémentation de la fonction `GMRES_v2` qui va exploiter les fonctions `Step_Arnoldi_h` et `QR_Givens_h` afin de ne plus stocker la matrice `H` et de ne plus refaire, à chaque itération, la factorisation QR complète de `H`. On reprendra soin de bien définir un vecteur `g` qui sera initialisé comme un scalaire dont la valeur est la norme de $r_0$ et auquel on ajoutera, à chaque itération, un coefficient $0$ (voir `np.vstack`) puis on le passera en argument à la fonction `QR_Givens_h` (comme 3ème argument).\n",
    "\n",
    "> **A faire :** Implémenter la fonction `GMRES_v2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMRES_v2(A,b,x0 = None,tol = 1e-10,itermax = None):\n",
    "    if x0 is None:\n",
    "        x0 = np.zeros((len(b),1))\n",
    "    if itermax is None:\n",
    "        itermax = len(b)\n",
    "    b = b.reshape((len(b),1))\n",
    "    r = b - A.dot(x0)\n",
    "    beta = npl.norm(r)\n",
    "    rho = beta\n",
    "    g = np.array([beta])\n",
    "    Q = r.reshape((len(b),1))/float(beta)\n",
    "    O = None\n",
    "    niter = 1\n",
    "    while (niter<itermax) and (rho>tol):\n",
    "        g = np.vstack((g, 0.))\n",
    "        Q,h = Step_Arnoldi_h(A,Q)\n",
    "        O,h,g = QR_Givens_h(O,h,g)\n",
    "        if niter == 1:\n",
    "            T = h[-2].reshape((1,1))\n",
    "        else:\n",
    "            m = np.zeros((1,niter-1))\n",
    "            T = np.concatenate((T,m),axis = 0)\n",
    "            T = np.concatenate((T,h[:-1]),axis = 1)\n",
    "        rho = abs(g[-1])\n",
    "        niter += 1\n",
    "    y = npl.solve(T,g[:-1]).reshape(niter-1,1)\n",
    "    x = x0 + Q[:,:niter-1].dot(y)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **A faire :** Testez votre code avec un système linéaire de taille 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = RandMat(50)\n",
    "b = np.random.random(50)\n",
    "x_exact = npl.solve(A,b)\n",
    "x = GMRES_v2(A,b)\n",
    "print(npl.norm(x-x_exact.reshape(50,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. L'équation de Burgers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'intéresse maintenant à l'équation de Burgers qui est donnée par\n",
    "\n",
    "$$ \\left\\{\\begin{array}{ll}\\partial_t u(t,x) + u(t,x)\\partial_x u(t,x) = \\mu \\partial^2_x u(t,x), \\forall (t,x)\\in ]0,T]\\times[0,1],\\\\ u(t,0) = u(t,1) = 0, \\forall t\\in\\mathbb{R}^+, \\\\ u(0,x) = v(x), \\forall x\\in[0,1].\\end{array}\\right.$$\n",
    "\n",
    "On commence par discrétiser l'équation en temps. Pour cela, on choisit un schéma d'Euler semi-implicite qui apporte un compromis entre Euler explicite et Euler implicite. Pour un pas en temps uniforme $s>0$, celui-ci nous amène à l'équation discrétisée en temps suivante\n",
    "\n",
    "$$\\left\\{\\begin{array}{ll} u_{n+1}(x) - u_n(x) + s u_n(x)\\partial_x u_{n+1}(x) = s\\mu \\partial^2_x u_{n+1}(x), \\quad\\forall x\\in[0,1],\\\\ u_{n+1}(0) = u_{n+1}(1) = 0,\\\\ u_0(x) = v(x), \\forall x\\in[0,1].\\end{array}\\right.$$\n",
    "\n",
    "On passe maintenant à la discrétisation en espace. L'intervalle $[0,1]$ est discrétisé en $n+2$ points ($2$ points consistuent les bords). La discrétisation de l'opérateur de Laplace se fait par des différences finies centrées du second ordre et de pas $h = \\frac 1 {n+1}$ (avec conditions aux bords de Dirichlet). On en déduit la matrice $L\\in M_{n,n}(\\mathbb{R})$ associée\n",
    "\n",
    "$$ (L)_{i,j} =\\left\\{\\begin{array}{ll} -\\frac 2 {h^2} \\mbox{ lorsque } i=j,\\\\ \\frac1{h^2} \\mbox{ lorsque } i=j+1 \\mbox{ ou } j = i+1\\\\ 0 \\mbox{ sinon.} \\end{array}\\right. $$\n",
    "\n",
    "Pour l'opérateur gradient, on peut choisir des différences finies du second ordre centrées, à droite ou à gauche. On obtient $G_0$, $G_1$ et $G_{-1}\\in M_{n,n}(\\mathbb{R})$ données par:\n",
    "\n",
    "$$ (G_0)_{i,j} =\\left\\{\\begin{array}{ll} -\\frac1{2h} \\mbox{ lorsque } i=j+1,\\\\ \\frac1{2h} \\mbox{ lorsque } j=i+1,\\\\ 0 \\mbox{ sinon.} \\end{array}\\right. $$\n",
    "\n",
    "$$ (G_1)_{i,j} =\\left\\{\\begin{array}{ll} -\\frac3{2h} \\mbox{ lorsque } j=i,\\\\ \\frac2{h} \\mbox{ lorsque } j=i+1,\\\\ -\\frac1{2h} \\mbox{ lorsque } j=i+2,\\\\ 0 \\mbox{ sinon.} \\end{array}\\right. $$\n",
    "et\n",
    "$$ (G_{-1})_{i,j} =\\left\\{\\begin{array}{ll} \\frac3{2h} \\mbox{ lorsque } i=j,\\\\ -\\frac2{h} \\mbox{ lorsque } i=j+1,\\\\ \\frac1{2h} \\mbox{ lorsque } i=j+2,\\\\ 0 \\mbox{ sinon.} \\end{array}\\right. $$\n",
    "\n",
    "L'équation de Burgers discrétisée en temps et en espace devient donc\n",
    "$$\\left\\{\\begin{array}{ll} u_{n+1} - u_n + s U_n G_k u_{n+1} = s\\mu L u_{n+1},\\\\ u_0 = v,\\end{array}\\right.$$\n",
    "\n",
    "où $k\\in \\{-1,0,1\\}$ et où l'on note $U_n$ la matrice diagonale de taille $n$ telle que\n",
    "$$ \\left(U_n\\right)_{i,j} = \\left\\{\\begin{array}{ll} (u_n)_i \\mbox{ lorsque } i=j,\n",
    "\\\\ 0 \\mbox{ sinon.}\\end{array}\\right. $$ \n",
    "\n",
    "On peut réécrire la première équation sous la forme d'un système linéaire\n",
    "$$ \\left(\\mbox{Id} + sU_nG_k - s\\mu L\\right) u_{n+1} = u_n ,$$ \n",
    "\n",
    "dont la solution est l'itérée suivante dans notre schéma d'Euler semi-implicite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0. Définition des matrices et de la donnée initiale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v(x):\n",
    "    d = 1e-1\n",
    "    return np.exp(-(x-0.25)**2/d**2)\n",
    "def Laplace_matrix(n):\n",
    "    return -scs.diags(2*np.ones(n),0) + scs.diags(np.ones(n-1),1) + scs.diags(np.ones(n-1),-1)\n",
    "def Gradient_matrix(n,k=0):\n",
    "    if (k == 0):\n",
    "        G = scs.diags(-0.5*np.ones(n-1),-1) + scs.diags(0.5*np.ones(n-1),1)\n",
    "    elif (k==1):\n",
    "        G = scs.diags(-1.5*scs.ones(n),0) + scs.diags(2*np.ones(n-1),1) + scs.diags(-0.5*np.ones(n-2),2)\n",
    "    elif (k==-1):\n",
    "        G = scs.diags(1.5*np.ones(n),0) + scs.diags(-2*np.ones(n-1),-1) + scs.diags(0.5*np.ones(n-2),-2)\n",
    "    return G\n",
    "def Un_matrix(un):\n",
    "    return scs.diags(un[:,0],0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Simulation de l'équation de Burgers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au vu des performances de la méthode CGNR (et de sa sensibilité au conditionnement), on se propose de n'utiliser que la méthode GMRES pour résoudre le système linéaire. D'autre part, on pourra se baser sur les paramètres suivants:\n",
    "- $h = 1/(n+1)$ avec $n = 200$,\n",
    "- $T = 2$, $s = T/m$ avec $m = 200$,\n",
    "- $\\mu = 10^{-3}$,\n",
    "- $k = 0$ (différences finies centrées pour le gradient).\n",
    "\n",
    "Afin de tracer la solution, on pourra utiliser la fonction $plt.pcolor$ afin de faire un tracer de la solution en $2$ dimensions: le temps et l'espace. Enfin, il est intéressant de tester l'influence de la valeur du paramètre $\\mu$ lorsque celui-ci devient petit en fonction des $3$ discrétisations de l'opérateur gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Burgers(v,n,m,T = 1,mu = 1e-7, k = 0):\n",
    "    x = np.linspace(0,1,n+2)\n",
    "    u = v(x)\n",
    "    x = x[1:n+1]\n",
    "    u = u[1:n+1].reshape((n,1))\n",
    "    U = np.copy(u)\n",
    "    h = 1./(n+1)\n",
    "    s = float(T)/m\n",
    "    L = Laplace_matrix(n)\n",
    "    G = (s/h)*Gradient_matrix(n,k)\n",
    "    I = np.eye(n)\n",
    "    A0 = I - (mu*s/h**2)*L\n",
    "    t = np.linspace(0,T,m+1)\n",
    "    for i in range(m):\n",
    "        A = A0 + Un_matrix(np.array(u))*G\n",
    "        u = GMRES_v2(A,u)\n",
    "        U = np.concatenate((U,np.copy(u)),axis = 1)\n",
    "    T,X = np.meshgrid(t,x)\n",
    "    return U,T,X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "m = 200\n",
    "\n",
    "U,T,X = Burgers(v,n,m,T = 5,k=-1)\n",
    "plt.pcolor(T,X,np.array(U))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
