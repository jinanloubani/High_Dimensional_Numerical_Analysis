{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problème de Poisson, méthodes itératives \"basiques\" et le gradient conjugué"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce travail, on se propose de comparer les différentes méthodes de résolution itératives \"basiques\" avec l'algorithme du gradient conjugué pour la résolution d'un système linéaire dont la matrice est symétrique définie positive. Plus précisément, on va comparer: la méthode de Jacobi, la méthode de Gauss-Siedel et la méthode de plus profonde descente avec le gradient conjugué. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Un petit problème pour commencer (et visualiser les choses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va débuter avec un problème de taille $2\\times 2$ afin de tester les méthodes et voir leur comportement. Pour cela, on va considérer le problème suivant:\n",
    "\n",
    "$$ Ax=b,$$\n",
    "\n",
    "où $A = \\begin{pmatrix}3 & 2 \\\\ 2 & 6 \\end{pmatrix}$ et $ b = \\begin{pmatrix} 2 \\\\ -8 \\end{pmatrix}$. On remarque que la matrice $A$ est symétrique définie positive et que la solution $x$ est aussi la solution du problème de minimisation dans $\\mathbb{R}^2$\n",
    "\n",
    "$$ x = \\mbox{argmin}_{y\\in\\mathbb{R}^2}\\frac12 \\langle y,Ay\\rangle - \\langle b,y\\rangle. $$\n",
    "\n",
    "On se propose tout d'abord d'implémenter les méthodes de Jacobi et Gauss-Siedel pour la résolution du système linéaire  et méthode de plus profonde descente pour le problème de minimisation. On veillera à ce que les méthodes soient appliquées via des fonctions qui prendront en entrée une matrice $A\\in M_{n,n}(\\mathbb{R})$ (taille quelconque), un vecteur $b\\in\\mathbb{R}^n$, un point de départ $x_0\\in\\mathbb{R}^n$ et une tolérance pour le critère d'arrêt. On basera ce dernier sur la norme euclidienne de la différence entre $2$ itérés (sauf pour le gradient conjugué). Finalement, on pourra aussi désigner une variable d'entrée pour la méthode de Gauss-Siedel afin d'indiquer la décomposition choisie (triangulaire inférieure ou supérieure). Finalement, il sera préférable (dans un premier temps) de donner en sortie l'ensemble des points calculés par les méthodes afin d'afficher leur évolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0. Chargement des librairies classiques pour le calcul scientifique et rappels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as npl\n",
    "import scipy.linalg as spl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Affichage pratique de la fonction f(x) = 1/2*<x,Ax> - <b,x>\n",
    "def affichage(P) :\n",
    "    Nx = 1000\n",
    "    Ny = 1000\n",
    "    xmin = np.min(P[:,0]) - 0.5\n",
    "    xmax = np.max(P[:,0]) + 0.5\n",
    "    ymin = np.min(P[:,1]) - 0.5\n",
    "    ymax = np.max(P[:,1]) + 0.5\n",
    "    x = np.linspace(xmin,xmax,Nx)\n",
    "    y = np.linspace(ymin,ymax,Ny)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z= 0.5*(3*X**2 + 4*X*Y + 6*Y**2) - 2*X + 8*Y\n",
    "    V = [k/10.*np.max(Z) + (1-k/10.)*np.min(Z) for k in range(11)]\n",
    "    CS=plt.contour(X, Y, Z, V,colors='k')\n",
    "    plt.scatter(P[:,0], P[:,1],marker='o')\n",
    "    plt.clabel(CS, inline=1, fontsize=10)\n",
    "    plt.show()\n",
    "P = np.array([[0,0],[-0.5,0.5],[0.5,-0.5],[1,-1]])\n",
    "affichage(P)\n",
    "\n",
    "# Résolution d'un système linéaire\n",
    "A = np.array([[3,2],[2,6]])\n",
    "b = np.array([2,-8])\n",
    "x = npl.solve(A,b)\n",
    "\n",
    "# Résolution d'un système triangulaire inférieur\n",
    "T = np.array([[3,0],[2,6]])\n",
    "x = spl.solve_triangular(T,b,lower = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Implémentation de la méthode de Jacobi et Gauss-Siedel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jacobi(A,b,x0 = None,tol = 1e-10):\n",
    "    if x0 is None:\n",
    "        x0 = np.zeros(len(b))\n",
    "    x = np.copy(x0)\n",
    "    x_tmp = np.copy(x)\n",
    "    delta = 2*tol\n",
    "    M = np.diag(np.diag(A),0)\n",
    "    N = M - A\n",
    "    M_inv = np.diag(1./np.diag(A),0)\n",
    "    z = np.dot(M_inv,b)\n",
    "    Xiter = [x0]\n",
    "    while delta > tol:\n",
    "        x_tmp = np.copy(x)\n",
    "        x = np.dot(M_inv,np.dot(N,x)) + z\n",
    "        delta = npl.norm(x-x_tmp)\n",
    "        Xiter += [x]\n",
    "    return np.array(Xiter)\n",
    "\n",
    "def Gauss_Siedel(A,b,x0 = None,tol = 1e-10):\n",
    "    if x0 is None:\n",
    "        x0 = np.zeros(len(b))\n",
    "    x = np.copy(x0)\n",
    "    x_tmp = np.copy(x)\n",
    "    delta = 2*tol\n",
    "    M = np.triu(A,0)\n",
    "    N = M - A\n",
    "    z = spl.solve_triangular(M,b, lower = False)\n",
    "    Xiter = [x0]\n",
    "    while delta > tol:\n",
    "        x_tmp = np.copy(x)\n",
    "        x = spl.solve_triangular(M,np.dot(N,x), lower = False) + z\n",
    "        delta = npl.norm(x-x_tmp)\n",
    "        Xiter += [x]\n",
    "    return np.array(Xiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Implémentation de la méthode de plus profonde descente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rappelle que la méthode de plus profonde descente est un algorithme de type gradient dont le pas est optimisé. La méthode s'écrit donc sous la forme\n",
    "\n",
    "$$ \\left\\{\\begin{array}{ll} x_0\\in\\mathbb{R}^n, \\\\ x_{k+1} = x_k - s_k \\nabla f(x_k).  \\end{array}\\right. $$\n",
    "\n",
    "où $f(x) = \\frac12 \\langle x,Ax\\rangle - \\langle b,x\\rangle$ et donc $- \\nabla f(x_k) = b - Ax_k = r_k$ (qui correspond donc au résidu associé à $x_k$). À chaque itération, $s_k$ est choisit tel que\n",
    "\n",
    "$$ s_k = \\mbox{argmin}_{s\\in\\mathbb{R}} f(x_k - s \\nabla f(x_k)). $$\n",
    "\n",
    "> **A faire :** Montrer, en dérivant l'expression $f(x_k - s \\nabla f(x_k))$ par rapport à $s$, que $s_k$ est explicitement donné par la formule\n",
    "$$ s_k = \\frac{\\|r_k\\|_2^2}{\\langle A r_k,r_k\\rangle}. $$\n",
    "En déduire que les directions de descentes sont orthogonales une à une, c'est-à-dire que\n",
    "$$ <r_{k+1},r_k> = 0.$$\n",
    "\n",
    "On en déduit donc que la méthode se décompose comme\n",
    "\n",
    "$$ \\left\\{\\begin{array}{ll} x_0\\in\\mathbb{R}^n, \\\\ r_k = b - Ax_k, \\\\ s_k = \\frac{\\|r_k\\|_2^2}{\\langle A r_k,r_k\\rangle}, \\\\ x_{k+1} = x_k + s_k r_k.  \\end{array}\\right. $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Steepest_descent(A,b,x0 = None,tol = 1e-10):\n",
    "    if x0 is None:\n",
    "        x0 = np.zeros(len(b))\n",
    "    x = np.copy(x0)\n",
    "    x_tmp = np.copy(x)\n",
    "    delta = 2*tol\n",
    "    Xiter = [x0]\n",
    "    while delta>tol:\n",
    "        x_tmp = np.copy(x)\n",
    "        r = b-np.dot(A,x)\n",
    "        s = float(npl.norm(r)**2)/np.dot(np.dot(A,r),r)\n",
    "        x = x + s*r\n",
    "        delta = npl.norm(x-x_tmp)\n",
    "        Xiter += [x]\n",
    "    return np.array(Xiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Implémentation du gradient conjugué"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée pour améliorer la convergence de la méthode de la plus profonde descente consiste à construire des directions de descentes qui ne sont plus simplement orthogonales une à une mais $A$-conjuguées. On aboutit alors à l'algorithme du gradient conjugué que l'on a vu en cours (sous une approche différente). Le fait d'avoir une famille de direction $A$-conjuguées va dramatiquement améliorer la convergence de la méthode de gradient. Le critère d'arrêt sera, cette fois, basé sur la norme euclidienne du résidu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conjugate_Gradient(A,b,x0 = None,tol = 1e-10):\n",
    "    if x0 is None:\n",
    "        x0 = np.zeros(len(b))\n",
    "    x = np.copy(x0)\n",
    "    x_tmp = np.copy(x)\n",
    "    delta = 2*tol\n",
    "    Xiter = [x0]\n",
    "    r = b-np.dot(A,x)\n",
    "    r_tmp = np.copy(r)\n",
    "    d = np.copy(r)\n",
    "    while npl.norm(r)>tol:\n",
    "        r_tmp = np.copy(r)\n",
    "        t = float(npl.norm(r)**2)/np.dot(np.dot(A,d),d)\n",
    "        x = x + t*d\n",
    "        r = r - t*np.dot(A,d)\n",
    "        if npl.norm(r)>1e-15:\n",
    "            s = float(npl.norm(r)**2)/npl.norm(r_tmp)**2\n",
    "            d = r + s*d\n",
    "        Xiter += [x]\n",
    "    return np.array(Xiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Comparaison du comportement des différentes méthodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **A faire :** À l'aide de la fonction d'affichage que l'on fournit, tester les différentes méthodes et commenter les résultats obtenus pour différents points initiaux. On pourra par exemple prendre comme points de départ:\n",
    "- x1 = [0,0]\n",
    "- x2 = [2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"TEST: x0 = [0,0]\\n\"\n",
    "xini = np.array([0,0])\n",
    "XJacobi = Jacobi(A,b,xini)\n",
    "print \"Méthode de Jacobi (avec\",len(XJacobi[:,0])-1,\"itérations):\"\n",
    "affichage(XJacobi)\n",
    "XGauss_Siedel = Gauss_Siedel(A,b,xini)\n",
    "print \"Méthode de Gauss-Siedel (avec\",len(XGauss_Siedel[:,0])-1,\"itérations):\"\n",
    "affichage(XGauss_Siedel)\n",
    "XSteepest_descent = Steepest_descent(A,b,xini)\n",
    "print \"Méthode de plus profonde descente (avec\",len(XSteepest_descent[:,0])-1,\"itérations):\"\n",
    "affichage(XSteepest_descent)\n",
    "XConjugate_Gradient = Conjugate_Gradient(A,b,xini)\n",
    "print \"Méthode du gradient conjugué (avec\",len(XConjugate_Gradient[:,0])-1,\"itérations):\"\n",
    "affichage(XConjugate_Gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Le problème de Poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'intéresse maintenant au problème de Poisson en dimension $1$ suivant\n",
    "\n",
    "$$\\left\\{\\begin{array}{ll} -\\Delta u(\\ell) = v(\\ell), \\quad \\forall \\ell\\in ]0,1[, \\\\ u(0) = u(1) = 0\\end{array}\\right., $$\n",
    "\n",
    "où $v$ est une fonction que l'on choisira comme étant\n",
    "\n",
    "$$ v(\\ell) = \\left\\{\\begin{array}{ll} 0 \\mbox{ pour } \\ell\\in[0,1/3[\\cap]2/3,1], \\\\ 1 \\mbox{ pour } \\ell\\in [1/3,2/3]. \\end{array}\\right.$$\n",
    "\n",
    "On discrétise l'opérateur de Laplace par des différences finies du second ordre de pas $h = \\frac 1 {n+1}$ (avec conditions aux bords de Dirichlet) et l'on en déduit la matrice $A\\in M_{n,n}(\\mathbb{R})$ associée\n",
    "\n",
    "$$ A =\\left(\\begin{array}{ccccc} 2 & -1 & 0 & \\dots & 0 \\\\ -1 & 2 & -1 & \\ddots & \\vdots \\\\ 0 & \\ddots & \\ddots & \\ddots & 0\\\\ \\vdots & \\ddots & -1 & 2  & -1 \\\\ 0 & \\dots & 0 & -1 & 2\\end{array}\\right). $$\n",
    "\n",
    "et donc le système linéaire\n",
    "\n",
    "$$ Ax = b, $$\n",
    "\n",
    "où $b\\in\\mathbb{R}^n$ est donné par\n",
    "\n",
    "$$ b_i = \\left\\{ \\begin{array}{ll} h^2 \\mbox{ pour } i\\in \\left[\\frac{n+1}3,\\frac{2(n+1)}3\\right] \\\\ 0 \\mbox{ sinon.} \\end{array}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0. Définition de la matrice A et du vecteur b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = np.linspace(0,1,n+2)\n",
    "h = x[1] - x[0]\n",
    "def v(x):\n",
    "    v = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        if (x[i]>1./3) and (x[i]<2./3):\n",
    "            v[i] = 1\n",
    "    return v\n",
    "def Poisson_discrete(n,v):\n",
    "    A = np.diag(2*np.ones(n),0)-np.diag(np.ones(n-1),1)-np.diag(np.ones(n-1),-1)\n",
    "    x = np.linspace(0,1,n+2)\n",
    "    h = x[1] - x[0]\n",
    "    b = h**2*v(x[1:n+1])\n",
    "    return A,b\n",
    "A,b = Poisson_discrete(n,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Résolution du système par la méthode np.solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par la suite, on considérera (à raison?) que la solution exacte $x$ du problème est celle donnée par la méthode $np.solve$ de Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.zeros(n+2)\n",
    "u[1:n+1] = npl.solve(A,b)\n",
    "plt.plot(x,u)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Comparaison des méthodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On se propose de comparer les méthodes en traçant l'évolution des quantités suivantes au fur et à mesure des itérations:\n",
    "- la norme $A$ de l'erreur $$e_m(A) = \\|x_m-x\\|_{A} = \\langle x_m-x,A(x_m-x)\\rangle^{1/2},$$\n",
    "- la norme euclidienne de l'erreur $$e_m(2) =\\|x_m-x\\|_2,$$\n",
    "- et la norme euclidienne du résidu $$e_m(R) = \\|r_m\\|_2.$$\n",
    "\n",
    "> **A faire :** Tracer l'évolution de $e_m(A)$, $e_m(2)$ et $e_m(R)$ pour la méthode de Jacobi, Gauss-Siedel, plus profonde descente et le gradient conjugué en partant du point $x_0 = 0$. Commenter les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XJacobi = Jacobi(A,b)\n",
    "XGauss_Siedel = Gauss_Siedel(A,b)\n",
    "XSteepest_descent = Steepest_descent(A,b)\n",
    "XConjugate_Gradient = Conjugate_Gradient(A,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Error_A(Xiter,x,A):\n",
    "    E = np.zeros(len(Xiter[:,0]))\n",
    "    for i in range(len(Xiter[:,0])):\n",
    "        E[i] = np.dot(Xiter[i,:] - x,np.dot(A,Xiter[i,:] - x))\n",
    "    return E\n",
    "def Error_2(Xiter,x,A):\n",
    "    E = np.zeros(len(Xiter[:,0]))\n",
    "    for i in range(len(Xiter[:,0])):\n",
    "        E[i] = npl.norm(Xiter[i,:] - x)\n",
    "    return E\n",
    "def Error_R(Xiter,b,A):\n",
    "    E = np.zeros(len(Xiter[:,0]))\n",
    "    for i in range(len(Xiter[:,0])):\n",
    "        E[i] = npl.norm(b - np.dot(A,Xiter[i,:]))\n",
    "    return E\n",
    "X = npl.solve(A,b)\n",
    "\n",
    "EJacobi_A = Error_A(XJacobi,X,A)\n",
    "EJacobi_2 = Error_2(XJacobi,X,A)\n",
    "EJacobi_R = Error_R(XJacobi,b,A)\n",
    "EGauss_Siedel_A = Error_A(XGauss_Siedel,X,A)\n",
    "EGauss_Siedel_2 = Error_2(XGauss_Siedel,X,A)\n",
    "EGauss_Siedel_R = Error_R(XGauss_Siedel,b,A)\n",
    "ESteepest_descent_A = Error_A(XSteepest_descent,X,A)\n",
    "ESteepest_descent_2 = Error_2(XSteepest_descent,X,A)\n",
    "ESteepest_descent_R = Error_R(XSteepest_descent,b,A)\n",
    "EConjugate_Gradient_A = Error_A(XConjugate_Gradient,X,A)\n",
    "EConjugate_Gradient_2 = Error_2(XConjugate_Gradient,X,A)\n",
    "EConjugate_Gradient_R = Error_R(XConjugate_Gradient,b,A)\n",
    "\n",
    "plt.plot(np.log10(EJacobi_A), label = 'E(A)')\n",
    "plt.plot(np.log10(EJacobi_2), label = 'E(2)')\n",
    "plt.plot(np.log10(EJacobi_R), label = 'E(R)')\n",
    "plt.title('Methode de Jacobi')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.log10(EGauss_Siedel_A), label = 'E(A)')\n",
    "plt.plot(np.log10(EGauss_Siedel_2), label = 'E(2)')\n",
    "plt.plot(np.log10(EGauss_Siedel_R), label = 'E(R)')\n",
    "plt.title('Methode de Gauss-Siedel')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.log10(ESteepest_descent_A), label = 'E(A)')\n",
    "plt.plot(np.log10(ESteepest_descent_2), label = 'E(2)')\n",
    "plt.plot(np.log10(ESteepest_descent_R), label = 'E(R)')\n",
    "plt.title('Methode de plus profonde descente')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.log10(EConjugate_Gradient_A), label = 'E(A)')\n",
    "plt.plot(np.log10(EConjugate_Gradient_2), label = 'E(2)')\n",
    "plt.plot(np.log10(EConjugate_Gradient_R), label = 'E(R)')\n",
    "plt.title('Methode du gradient conjugue')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
